{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 9.82MB/s]                    \n",
      "2024-04-11 19:06:45 INFO: Downloaded file to /Users/omarmhawash/stanza_resources/resources.json\n",
      "2024-04-11 19:06:45 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-04-11 19:06:46 INFO: File exists: /Users/omarmhawash/stanza_resources/en/default.zip\n",
      "2024-04-11 19:06:50 INFO: Finished downloading models and saved to /Users/omarmhawash/stanza_resources\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/omarmhawash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/omarmhawash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stanza.download('en')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 19:06:51 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 7.59MB/s]                    \n",
      "2024-04-11 19:06:51 INFO: Downloaded file to /Users/omarmhawash/stanza_resources/resources.json\n",
      "2024-04-11 19:06:51 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-04-11 19:06:51 INFO: Using device: cpu\n",
      "2024-04-11 19:06:51 INFO: Loading: tokenize\n",
      "2024-04-11 19:06:52 INFO: Loading: mwt\n",
      "2024-04-11 19:06:52 INFO: Loading: lemma\n",
      "2024-04-11 19:06:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#* pipeline for lemmatization and tokenization \n",
    "#? required: ['lemma', 'tokenize', 'mwt']\n",
    "processors = 'tokenize, lemma, mwt'\n",
    "\n",
    "tkn = stanza.Pipeline(lang='en', processors=processors, use_gpu=False)\n",
    "\n",
    "#* set of stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preparing dataset:\n",
    "**a.** formatting `classes` as a dict of text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_class {'files': ['0007261', '0000552', '0005647', '0001212', '0006258', '0009266', '0004855', '0005514', '0000413', '0000220', '0005308', '0005901', '0005895', '0002871', '0005261', '0005894', '0007882', '0008291']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "documents = os.listdir('data/training')\n",
    "classes = {}\n",
    "\n",
    "for i in range(len(documents)):\n",
    "  if documents[i] != '.DS_Store':\n",
    "    classes[documents[i]] = {\n",
    "      'files' : os.listdir(f'data/training/{documents[i]}'),\n",
    "    }\n",
    "\n",
    "# example output\n",
    "t_class = 'tin'\n",
    "print(f't_class',classes[t_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preparing dataset:\n",
    "**b.** creating a `train` dataframe. Also `texts` for building the vocabulary set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': 'tin',\n",
       " 'text': 'thai tin exports fall february bangkok march 27 thailand exported 120 tonnes tin metal february 816 tonnes previous month 140 tonnes year ago mineral resources department said said major buyers last month britain japan netherlands west germany'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "train = []\n",
    "texts = []\n",
    "\n",
    "for c in classes:\n",
    "  for idx, f in enumerate(classes[c]['files']):\n",
    "    with open(f'data/training/{c}/{f}', 'r') as file:\n",
    "      filedata = file.read()\n",
    "      mod_data = filedata.replace('\\n', ' ').lower() #? lowercase here\n",
    "      split_mod_data = re.split('[ ,.\\'\\\"><]+', mod_data)\n",
    "      filtered_data = ' '.join([w for w in split_mod_data if (not w in stop_words) and (len(w) > 1)])\n",
    "      texts.append(filtered_data)\n",
    "      train.append({'class': c, 'text': filtered_data })\n",
    "    if idx > 1000:\n",
    "      break\n",
    "\n",
    "train[0] #? sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    thai tin exports fall february bangkok march 2...\n",
       "1    zaire accepts tin-export quota atpc says kuala...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #* creating train dataframe\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train)\n",
    "\n",
    "# print(train_df['class'].value_counts())\n",
    "train_df['text'].head(2) #? sample output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.\tTokenization and Vocabulary set extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x111ccda10>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/omarmhawash/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# #* formatting to 'stanza' documents\n",
    "in_docs = [stanza.Document([], text=d) for d in texts]\n",
    "\n",
    "#* tokenization pipeline\n",
    "tkn_docs = tkn(in_docs) #? time consuming...~ 4-6mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "new_lemmas = [[token.lemma_ for token in nlp(d)] for d in texts]\n",
    "new_flat_lemmas = [lemma for sublist in new_lemmas for lemma in sublist]\n",
    "\n",
    "new_flat_lemmas[:10] #? sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words: 1202487, All vocab: 25123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#* taking out tokens and flattening the sentences\n",
    "lemmas = [[word.lemma for word in sentence.words] for doc in tkn_docs for sentence in doc.sentences] \n",
    "flat_lemma = [item for sublist in lemmas for item in sublist]\n",
    "\n",
    "#* creating a count vectorizer\n",
    "cn_vec = CountVectorizer()\n",
    "cn = cn_vec.fit_transform(flat_lemma)\n",
    "\n",
    "#? counting lemmas then finding the total words and vocab size\n",
    "all_words = len(flat_lemma)\n",
    "all_vocab = len(cn_vec.get_feature_names_out())\n",
    "\n",
    "print(f'All words: {all_words}, All vocab: {all_vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thai', 'tin', 'export', 'fall', 'february', 'bangkok', 'march', '27', 'Thailand', 'exported', '120', 'tonne', 'tin', 'metal', 'february', '816', 'tonne', 'previous', 'month', '140', 'tonne', 'year', 'ago', 'mineral', 'resource', 'department', 'say', 'say', 'major', 'buyer', 'last', 'month', 'britain', 'japan', 'netherlands', 'west', 'germany', 'zaire', 'accept', 'tin', '-', 'export', 'quota', 'atpc', 'say', 'kuala', 'lumpur', 'march', 'zaire', 'agree', 'limit', 'tin', 'export', '736', 'tonne', '12', 'month', 'march', 'line', 'association', 'tin', 'produce', 'country', '(', 'atpc', ')', 'plan', 'curb', 'export', 'atpc', 'say', 'atpc', 'executive', 'director', 'victor', 'siaahan', 'tell', 'reuter', 'receive', 'telex', 'zaire', 'indicate', 'willingess', 'take', 'part', 'plan', 'limit', 'total', 'atpc', 'export', '96', '000', 'tonne', 'year', 'march', 'siaahan', 'say', 'zaire', 'expect', 'produce']\n"
     ]
    }
   ],
   "source": [
    "# print last 100 words\n",
    "print(flat_lemma[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Building estimating model using naive bayes classifier method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/wrhvx0tx3639dzq6nh6bmx6r0000gq/T/ipykernel_1193/842227278.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_word_count = train_df.groupby('class').apply(class_words) #? takes about 30 seconds\n"
     ]
    }
   ],
   "source": [
    "# #* finding priors by:\n",
    "#? (class count according to total classes)\n",
    "#? then normalizing (0-1 scale)\n",
    "priors = train_df['class'].value_counts(normalize=True)\n",
    "\n",
    "#* finding the total count of each word across each class\n",
    "vec = CountVectorizer()\n",
    "\n",
    "def class_words(c):\n",
    "  '''\n",
    "  returns a dataframe of word counts for each class\n",
    "\n",
    "  c: dataframe. columns: [`class`, `text`]\n",
    "  '''\n",
    "  counts = vec.fit_transform(c['text'])\n",
    "  word_count = counts.sum(axis=0)\n",
    "  return pd.DataFrame(word_count, columns=vec.get_feature_names_out())\n",
    "\n",
    "df_word_count = train_df.groupby('class').apply(class_words) #? takes about 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acq'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #* creating a dictionary of words and their probabilities\n",
    "scale = 1 #? scale-up numbers factor\n",
    "\n",
    "word_prob_cache = {}\n",
    "\n",
    "def word_prob(word, c):\n",
    "  if (word, c) in word_prob_cache:\n",
    "    return word_prob_cache[(word, c)]\n",
    "\n",
    "  try:\n",
    "    word_sum = df_word_count[word][c].sum()\n",
    "  except:\n",
    "    word_sum = 0\n",
    "  try:\n",
    "    all_sum = df_word_count[word].sum()\n",
    "  except:\n",
    "    all_sum = all_words\n",
    "  res = (word_sum + 1) * scale / (all_sum + all_vocab)\n",
    "\n",
    "  word_prob_cache[(word, c)] = res\n",
    "  return res\n",
    "\n",
    "def prop_sentence(sentence, c):\n",
    "  '''\n",
    "  returns the probability of a sentence given a class\n",
    "  \n",
    "  sentence: str. sentence to find probability\n",
    "  c: str. class to find probability\n",
    "  '''\n",
    "  words = sentence.split(' ')\n",
    "  prob = 1\n",
    "  for word in words:\n",
    "    prob *= word_prob(word, c)\n",
    "  return prob * priors[c]\n",
    "\n",
    "def max_class(sentence)-> str:\n",
    "  '''\n",
    "  returns the class with the highest probability given a sentence\n",
    "  \n",
    "  sentence: str. sentence to find probability\n",
    "  '''\n",
    "  mod_data = sentence.replace('\\n', ' ').lower() #? lowercase here\n",
    "  split_mod_data = re.split('[ ,.\\'\\\"><]+', mod_data)\n",
    "  filtered_data = ' '.join([w for w in split_mod_data if (not w in stop_words) and (len(w) > 1)])\n",
    "  new_tokens = tkn(filtered_data)\n",
    "  new_sent = ' '.join([word.lemma for word in new_tokens.sentences[0].words])\n",
    "  probs = {}\n",
    "  for c in train_df['class'].unique():\n",
    "    probs[c] = prop_sentence(new_sent, c)\n",
    "  max_class = max(probs, key=probs.get)\n",
    "  return max_class\n",
    "\n",
    "#? sample output\n",
    "sentence = 'The president is a great leader'\n",
    "max_class(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Model evaluation: Macro-averaged mean score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #* new test data\n",
    "all_test = os.listdir('data/test')\n",
    "\n",
    "test_classes = {}\n",
    "for i in range(len(all_test)):\n",
    "  if all_test[i] != '.DS_Store':\n",
    "    test_classes[all_test[i]] = {\n",
    "      'files' : os.listdir(f'data/test/{all_test[i]}'),\n",
    "    }\n",
    "\n",
    "all_test = []\n",
    "for c in test_classes:\n",
    "  for idx, f in enumerate(test_classes[c]['files']):\n",
    "    with open(f'data/test/{c}/{f}', 'r', encoding='latin') as file:\n",
    "      filedata = file.read()\n",
    "      mod_data = filedata.replace('\\n', ' ').lower()#? lowercase here\n",
    "      split_mod_data = re.split('[ ,.\\'\\\"><]+', mod_data)\n",
    "      filtered_data = ' '.join([w for w in split_mod_data if (not w in stop_words) and (len(w) > 1)])\n",
    "      all_test.append({'class': c, 'text': filtered_data })\n",
    "\n",
    "all_test_df = pd.DataFrame(all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* testing the model using small test_df\n",
    "# stest_df = all_test_df.sample(500) #? takes about a minute to run\n",
    "stest_df = all_test_df #? takes about a minute to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! really long time to run\n",
    "stest_df['pred'] = stest_df['text'].apply(max_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.53876739562624%, F1: 13.396248702592306%\n"
     ]
    }
   ],
   "source": [
    "# using naive bayes from sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "#* creating a count vectorizer\n",
    "new_cn_vec = CountVectorizer()\n",
    "new_cn = new_cn_vec.fit_transform(train_df['text'])\n",
    "\n",
    "#* splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_cn, train_df['class'], test_size=0.001, random_state=42)\n",
    "\n",
    "#* getting test data\n",
    "X_test = new_cn_vec.transform(stest_df['text'])\n",
    "y_test = stest_df['class']\n",
    "\n",
    "#* fitting the model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "#* predicting the test data\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "#* accuracy and f1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy*100}%, F1: {f1*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 46.29721669980119%\n",
      "F1 Score: 3.774639744725782%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>soybean</td>\n",
       "      <td>supply/demand detailed usda washington april a...</td>\n",
       "      <td>grain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>grain</td>\n",
       "      <td>china daily says vermin eat 7-12 pct grain sto...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>acq</td>\n",
       "      <td>gulf western gw ups interest network new york ...</td>\n",
       "      <td>acq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>l-cattle</td>\n",
       "      <td>west virginia free two major cattle diseases w...</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>crude</td>\n",
       "      <td>coastal raises crude oil postings 50 cts/bbl y...</td>\n",
       "      <td>crude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>earn</td>\n",
       "      <td>altus bank alts 3rd qtr net mobile ala oct 19 ...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>acq</td>\n",
       "      <td>chase corp makes offer entregrowth wellington ...</td>\n",
       "      <td>tin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>acq</td>\n",
       "      <td>gander gndr buys western wear retailer wilmot ...</td>\n",
       "      <td>acq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>earn</td>\n",
       "      <td>dynamics research corp drco 1st qtr march 21 w...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>earn</td>\n",
       "      <td>westport bancorp webat 3rd qtr net westport co...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                               text     pred\n",
       "330    soybean  supply/demand detailed usda washington april a...    grain\n",
       "2099     grain  china daily says vermin eat 7-12 pct grain sto...  unknown\n",
       "3264       acq  gulf western gw ups interest network new york ...      acq\n",
       "12    l-cattle  west virginia free two major cattle diseases w...  unknown\n",
       "3922     crude  coastal raises crude oil postings 50 cts/bbl y...    crude\n",
       "1400      earn  altus bank alts 3rd qtr net mobile ala oct 19 ...     earn\n",
       "3531       acq  chase corp makes offer entregrowth wellington ...      tin\n",
       "3214       acq  gander gndr buys western wear retailer wilmot ...      acq\n",
       "1065      earn  dynamics research corp drco 1st qtr march 21 w...     earn\n",
       "800       earn  westport bancorp webat 3rd qtr net westport co...     earn"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "f1 = f1_score(stest_df['class'], stest_df['pred'], average='macro')\n",
    "acc = accuracy_score(stest_df['class'], stest_df['pred'])\n",
    "\n",
    "print(f'Mean Accuracy: {acc * 100}%')\n",
    "print(f'F1 Score: {f1 * 100}%')\n",
    "stest_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score p1:\n",
    "- 72.2\n",
    "- 20.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tEXTRA: think of new features that can be included into the Naïve Bayes Classifier, which contribute to improve the system performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
